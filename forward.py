import numpy as np


def initialize_parameters(layers_dim):
    """
    Initialisation of all layers weights and biases
    :param layers_dim: list of dims for each layer in the network
    :return:
     A dictionary containing the initialized W and b parameters of each layer (W1…WL, b1…bL).
    """
    ans = {
        "weights": [],
        "bias": []
    }
    for pair in zip(layers_dim, layers_dim[1:]):
        W = np.random.randn(*pair).T  # shape: [out_dim, in_dim]
        b = np.zeros((pair[-1], 1))
        ans["weights"].append(W)
        ans["bias"].append(b)

    return ans


def linear_forward(A, W, b):
    """
    Linear layer forward pass
    :param A: Output of the previous layer, shape size [previous_layer, 1]
    :param W: weights, shape size [current_layer, previous_layer]
    :param b: bias, shape size [current_layer, 1]
    :return:
    Z – the linear component of the activation function (i.e., the value before applying the non-linear function)
    linear_cache – a dictionary containing A, W, b (stored for making the backpropagation easier to compute)
    """
    Z = W @ A + b
    linear_cache = {
        "W": W,
        "A": A,
        "b": b
    }
    return Z, linear_cache


def softmax(Z):
    """
    Apply softmax function on Z
    :param Z: the linear component of the activation function
    :return:
    A – the activations of the layer
    activation_cache – returns Z, which will be useful for the backpropagation
    """
    A = np.exp(Z - np.amax(Z, 0, keepdims=True))  # numerical stability
    A = A / A.sum(0, keepdims=True)  # broadcasting
    activation_cache = {
        "Z": Z
    }
    return A, activation_cache


def relu(Z):
    """
    Apply relu function on Z
    :param Z:
    :return:
    """
    pred = lambda x: x if x > 0 else 0
    f = np.vectorize(pred, otypes=[float])
    A = f(Z)
    activation_cache = {
        "Z": Z
    }
    return A, activation_cache


def linear_activation_forward(A_prev, W, B, activation):
    """
    Implement the forward propagation for the LINEAR->ACTIVATION layer
    :param A_prev: activations of the previous layer
    :param W: the weights matrix of the current layer
    :param B: the bias vector of the current layer
    :param activation: the activation function to be used (a string, either “softmax” or “relu”)
    :return:
    """

    def select_activation(name):
        if name == "softmax":
            return softmax
        elif name == "relu":
            return relu

    Z, linear_cache = linear_forward(A_prev, W, B)
    A, activation_cache = select_activation(activation)(Z)
    return A, {**linear_cache, **activation_cache}


def L_model_forward(X, parameters, use_batchnorm: bool):
    """
    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX computation
    :param X: the data, numpy array of shape (input size, number of examples)
    :param parameters: the initialized W and b parameters of each layer
    :param use_batchnorm: a boolean flag used to determine whether to apply batchnorm after the activation
     (note that this option needs to be set to “false” in Section 3 and “true” in Section 4).
    :return:
    AL – the last post-activation value
    caches – a list of all the cache objects generated by the linear_forward function
    """

    cur_x = X
    L = len(parameters["weights"])
    caches = []
    for i, (W, b) in enumerate(zip(parameters["weights"], parameters["bias"])):
        activation = 'relu' if i < L - 1 else 'softmax'
        A, cache = linear_activation_forward(cur_x, W, b, activation)
        if use_batchnorm:
            A = apply_batchnorm(A)
        cur_x = A
        caches.append(cache)

    return cur_x, caches


def compute_cost(AL, Y):
    """
    Implement the cost function defined by equation.
    The requested cost function is categorical cross-entropy loss.
    :param AL: probability vector corresponding to your label predictions, shape (num_of_classes, number of examples)
    :param Y: the labels vector (i.e. the ground truth)
    :return: cost – the cross-entropy cost
    """

    def expand_Y(Y, l, m):
        C = np.zeros((l, m))
        for i, cl in enumerate(Y):
            C[cl, i] = 1
        return C

    # l, m = AL.shape
    # C = expand_Y(Y, l, m)
    m = len(Y)
    return - np.einsum('ij,ij', np.log(AL), Y) / m


def apply_batchnorm(A):
    """
    performs batchnorm on the received activation values of a given layer.
    :param A: The activation values of a given layer 
    :return: The normalized activation values, based on the formula learned in class
    """
    mean = np.expand_dims(A.mean(axis=1), axis=-1)
    var = np.expand_dims(A.var(axis=1), axis=-1)
    epsilon = 1e-4
    a_norm = (A - mean) / (var + epsilon)
    return a_norm
